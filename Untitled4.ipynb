{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LapJghfMkut",
        "outputId": "ab7a9106-2160-4d88-ef34-1990fddc5791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Co-teaching'...\n",
            "remote: Enumerating objects: 114, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 114 (delta 31), reused 29 (delta 29), pack-reused 76 (from 1)\u001b[K\n",
            "Receiving objects: 100% (114/114), 34.08 KiB | 1.70 MiB/s, done.\n",
            "Resolving deltas: 100% (56/56), done.\n"
          ]
        }
      ],
      "source": [
        "    !git clone https://github.com/bhanML/Co-teaching.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/UCSC-REAL/cifar-10-100n.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8jTTxyCMvQP",
        "outputId": "f47370a7-0281-4eb6-945b-ef162aea38a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cifar-10-100n'...\n",
            "remote: Enumerating objects: 157, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 157 (delta 46), reused 32 (delta 32), pack-reused 103 (from 2)\u001b[K\n",
            "Receiving objects: 100% (157/157), 3.57 MiB | 13.64 MiB/s, done.\n",
            "Resolving deltas: 100% (76/76), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c373f53b"
      },
      "source": [
        "**Reasoning**:\n",
        "The `SyntaxError` occurred in cell `39e6ecc0` because it was a Python code cell containing markdown text. To fix this, I will change the type of this cell to a markdown cell, allowing the instructions to be displayed correctly. After this, a new code cell will be provided for setting the API key.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e3f47af"
      },
      "source": [
        "# Task\n",
        "Set up a standard noisy training baseline for CIFAR-10N using a pre-trained ResNet-34 model. This includes importing necessary libraries, preparing CIFAR-10N (worst noisy labels) and clean CIFAR-10 datasets with data loaders and augmentations, defining the ResNet-34 model, implementing training and evaluation utilities, and finally training the model with SGD, a StepLR scheduler, and cross-entropy loss. Summarize the experimental setup and report the final test accuracy on the clean CIFAR-10 test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "283d4fb0"
      },
      "source": [
        "## Initial Setup and Imports\n",
        "\n",
        "### Subtask:\n",
        "Import essential libraries (torch, torchvision, numpy, etc.), detect the available device (CUDA if present), and set random seeds for reproducibility across the experiment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ecbb78c"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to import the necessary libraries, detect the available device (GPU or CPU), and set random seeds for reproducibility as outlined in the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cab74492",
        "outputId": "dafe836f-40a4-4176-f731-0130e75030d2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(f\"Random seeds set to {SEED} for reproducibility.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Random seeds set to 42 for reproducibility.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9caadc24"
      },
      "source": [
        "## Dataset and DataLoader Preparation\n",
        "\n",
        "### Subtask:\n",
        "Load the CIFAR-10 dataset, then modify its training labels using the 'worst' noisy labels from the 'cifar-10-100n' repository. Split the noisy training data into training and validation sets, and create data loaders for training, validation, and the clean CIFAR-10 test set. Implement standard data augmentations like random cropping and horizontal flipping, along with normalization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ec499c6"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to define standard image transformations for training and testing/validation. This involves using `torchvision.transforms` for data augmentation like random cropping, horizontal flipping, and normalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a197b3a9",
        "outputId": "43f93048-5bd1-4230-d4b5-6e1bc99f6a37"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 1. Define standard image transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "print(\"Defined image transformations for training and testing.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined image transformations for training and testing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a028e4c"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to load the clean CIFAR-10 training and test datasets using `torchvision.datasets.CIFAR10` as per instruction #2 of the subtask. I will set `download=True` to ensure the datasets are available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "845805cc",
        "outputId": "806ff45a-e0dd-4923-bf8c-7ced3a6ea7b4"
      },
      "source": [
        "import torchvision.datasets as datasets\n",
        "\n",
        "# 2. Load the clean CIFAR-10 training and test datasets\n",
        "cifar10_train_clean = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_test)\n",
        "cifar10_test_clean = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "print(f\"Clean CIFAR-10 training dataset loaded with {len(cifar10_train_clean)} samples.\")\n",
        "print(f\"Clean CIFAR-10 test dataset loaded with {len(cifar10_test_clean)} samples.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [07:56<00:00, 358kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean CIFAR-10 training dataset loaded with 50000 samples.\n",
            "Clean CIFAR-10 test dataset loaded with 10000 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f95d50bb"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to load the 'worst' noisy labels for CIFAR-10 from the specified path `cifar-10-100n/data/CIFAR-10_human.pt` and then replace the original training labels in the `cifar10_train_clean` dataset with these noisy labels, as per instructions #3 and #4 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9db0efa6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `torch.load` defaulted to `weights_only=True`, which is incompatible with the `.pt` file containing numpy objects. To resolve this, I will explicitly set `weights_only=False` in `torch.load` to allow the file to be loaded correctly, as suggested by the error message.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a4f2712"
      },
      "source": [
        "**Reasoning**:\n",
        "The `KeyError: 'worst_label'` occurred because the dictionary key used to access the noisy labels was incorrect. Based on the `noisy_labels` content from the kernel state, the correct key should be `'worse_label'`, not `'worst_label'`. I will correct this key in the code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c90ffd09",
        "outputId": "4270a3ef-f0ae-4e7b-b659-18a28a82cb46"
      },
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torch\n",
        "\n",
        "# 2. Load the clean CIFAR-10 training and test datasets\n",
        "cifar10_train_clean = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_test)\n",
        "cifar10_test_clean = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# 3. Load the 'worst' noisy labels for CIFAR-10\n",
        "# The file is CIFAR-10_human.pt in the cifar-10-100n/data directory\n",
        "noisy_labels = torch.load('./cifar-10-100n/data/CIFAR-10_human.pt', weights_only=False)\n",
        "# The 'worst' labels are typically under 'worse_label' key as seen in kernel state\n",
        "worst_noisy_labels = noisy_labels['worse_label']\n",
        "\n",
        "# 4. Replace the original training labels with the loaded noisy labels\n",
        "cifar10_train_clean.targets = worst_noisy_labels\n",
        "\n",
        "print(f\"Clean CIFAR-10 training dataset loaded with {len(cifar10_train_clean)} samples.\")\n",
        "print(f\"Clean CIFAR-10 test dataset loaded with {len(cifar10_test_clean)} samples.\")\n",
        "print(f\"CIFAR-10 training labels replaced with worst noisy labels. First 10 noisy labels: {worst_noisy_labels[:10]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean CIFAR-10 training dataset loaded with 50000 samples.\n",
            "Clean CIFAR-10 test dataset loaded with 10000 samples.\n",
            "CIFAR-10 training labels replaced with worst noisy labels. First 10 noisy labels: [4 9 0 5 1 9 2 4 8 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a3fa0b7"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to split the noisy CIFAR-10 training dataset into training and validation sets (80% train, 20% validation) and apply the `transform_train` to the training split and `transform_test` to the validation split, as instructed in #5 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28d2f221",
        "outputId": "5de557af-a6d8-4711-9c8b-b788451fbf35"
      },
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "# 5. Split the noisy CIFAR-10 training dataset into training and validation sets\n",
        "total_size = len(cifar10_train_clean)\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = total_size - train_size\n",
        "\n",
        "# Generate random indices for splitting\n",
        "indices = list(range(total_size))\n",
        "random.shuffle(indices)\n",
        "\n",
        "train_indices = indices[:train_size]\n",
        "val_indices = indices[train_size:]\n",
        "\n",
        "# Create subsets for training and validation\n",
        "cifar10_noisy_train_subset = Subset(cifar10_train_clean, train_indices)\n",
        "cifar10_noisy_val_subset = Subset(cifar10_train_clean, val_indices)\n",
        "\n",
        "# Apply transformations to the subsets\n",
        "# For the training subset, we modify its dataset's transform to transform_train\n",
        "cifar10_noisy_train_subset.dataset.transform = transform_train\n",
        "# For the validation subset, we modify its dataset's transform to transform_test\n",
        "cifar10_noisy_val_subset.dataset.transform = transform_test\n",
        "\n",
        "print(f\"Noisy training dataset split into {len(cifar10_noisy_train_subset)} training samples and {len(cifar10_noisy_val_subset)} validation samples.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noisy training dataset split into 40000 training samples and 10000 validation samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b84ae8ea"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to create `DataLoader` instances for the noisy training set, the validation set, and the clean CIFAR-10 test set, specifying appropriate batch sizes and shuffling for the training loader, as per instruction #6 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0e5533e",
        "outputId": "80ce619c-7ec0-4932-f22b-694701577c41"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 6. Create DataLoader instances\n",
        "BATCH_SIZE = 128\n",
        "TEST_BATCH_SIZE = 100\n",
        "\n",
        "train_loader_noisy = DataLoader(cifar10_noisy_train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader_noisy = DataLoader(cifar10_noisy_val_subset, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader_clean = DataLoader(cifar10_test_clean, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"DataLoaders created with batch sizes: Training={BATCH_SIZE}, Validation/Test={TEST_BATCH_SIZE}\")\n",
        "print(f\"Number of training batches: {len(train_loader_noisy)}\")\n",
        "print(f\"Number of validation batches: {len(val_loader_noisy)}\")\n",
        "print(f\"Number of test batches: {len(test_loader_clean)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders created with batch sizes: Training=128, Validation/Test=100\n",
            "Number of training batches: 313\n",
            "Number of validation batches: 100\n",
            "Number of test batches: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94d6a362"
      },
      "source": [
        "## Model Definition (ResNet-34)\n",
        "\n",
        "### Subtask:\n",
        "Define a Python function `build_resnet34(num_classes=10)` that constructs and returns a ResNet-34 model, pre-trained on ImageNet, with its final classification layer adapted for 10 classes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f19730e0"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to define the `build_resnet34` function that loads a pre-trained ResNet-34 model, modifies its final classification layer for 10 classes, and moves it to the detected device, as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "241c5f38"
      },
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "def build_resnet34(num_classes=10):\n",
        "    \"\"\"\n",
        "    Constructs and returns a ResNet-34 model, pre-trained on ImageNet,\n",
        "    with its final classification layer adapted for num_classes.\n",
        "    \"\"\"\n",
        "    # 3. Load a pre-trained ResNet-34 model\n",
        "    model = models.resnet34(pretrained=True)\n",
        "\n",
        "    # 4. Modify the final fully connected layer (fc) to output num_classes\n",
        "    # Get the number of in_features for the last layer\n",
        "    num_ftrs = model.fc.in_features\n",
        "    # Replace the last layer with a new one for num_classes\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    # 5. Move the model to the device\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(f\"ResNet-34 model built and adapted for {num_classes} classes. Moved to {device}.\")\n",
        "    # 6. Return the modified model\n",
        "    return model\n",
        "\n",
        "# Example usage (optional, for verification):\n",
        "# # Temporarily build the model to print its architecture and confirm modification\n",
        "# print(\"\\nVerifying model architecture (first 5 layers and final fc layer):\")\n",
        "# temp_model = build_resnet34()\n",
        "# print(list(temp_model.children())[:5])\n",
        "# print(temp_model.fc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6367ab5"
      },
      "source": [
        "**Reasoning**:\n",
        "The `build_resnet34` function has been defined. Now, I will instantiate the ResNet-34 model for 10 classes by calling this function, which will also move the model to the `device` and print a confirmation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d52934c1",
        "outputId": "1e92e1b2-f317-4d68-f71c-ec8d9ed2bc26"
      },
      "source": [
        "model = build_resnet34(num_classes=10)\n",
        "print(\"ResNet-34 model successfully instantiated.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 114MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet-34 model built and adapted for 10 classes. Moved to cpu.\n",
            "ResNet-34 model successfully instantiated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28801017"
      },
      "source": [
        "# Task\n",
        "The previous cell `cb3eea5d` failed with a `SyntaxError` because it was a code cell containing markdown text. I will correct this by changing its type to markdown. Then, I will provide a new code cell to implement the `train_one_epoch` and `evaluate` functions as described in the instructions.\n",
        "\n",
        "This will complete the \"Implement Training and Evaluation Functions\" part of the plan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef0803c1"
      },
      "source": [
        "## Implement Training and Evaluation Functions\n",
        "\n",
        "### Subtask:\n",
        "Implement functions for training and evaluating the model. The `train` function should perform a single epoch of training, including forward pass, loss calculation, backward pass, and optimizer step. The `evaluate` function should calculate the model's accuracy on a given DataLoader, switching the model to evaluation mode.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eab2ff08"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A `SyntaxError` in a previous cell was resolved by changing its type from code to markdown, indicating a correction in the document's structure.\n",
        "*   New code was implemented to define `train_one_epoch` and `evaluate` functions, which are essential for model training and performance assessment.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The implementation of `train_one_epoch` and `evaluate` functions sets up the core components for a complete training loop, preparing the groundwork for model development.\n",
        "*   The next logical step would be to integrate these functions into a full training script to train the model and monitor its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb3a2dc7"
      },
      "source": [
        "# Task\n",
        "Implement the `train_one_epoch` and `evaluate` functions, following the instructions to set up the model in training and evaluation modes, perform forward and backward passes, calculate loss, optimize, and compute accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b84ef5c8"
      },
      "source": [
        "## Implement General Training and Evaluation Utilities\n",
        "\n",
        "### Subtask:\n",
        "Implement functions for training and evaluating the model. The `train` function should perform a single epoch of training, including forward pass, loss calculation, backward pass, and optimizer step. The `evaluate` function should calculate the model's accuracy on a given DataLoader, switching the model to evaluation mode.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57b2bc82"
      },
      "source": [
        "**Reasoning**:\n",
        "I will implement the `train_one_epoch` and `evaluate` functions as described in the instructions, including setting model modes, handling data movement to the device, performing forward/backward passes, optimizing, and calculating loss and accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "318e64de",
        "outputId": "db0fc5e8-7d18-4acf-f83c-5e6257a2a898"
      },
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update model parameters\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_samples += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_accuracy = 100 * correct_predictions / total_samples\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Calculate loss\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / total_samples\n",
        "    accuracy = 100 * correct_predictions / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "print(\"Defined train_one_epoch and evaluate functions.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined train_one_epoch and evaluate functions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "341cd2c9"
      },
      "source": [
        "## Standard Noisy Training Baseline\n",
        "\n",
        "### Subtask:\n",
        "Implement a complete training loop for the standard noisy training baseline. This loop will train a ResNet-34 model on the noisy CIFAR-10N dataset using standard cross-entropy, SGD with momentum, and a StepLR scheduler for a configurable number of epochs. It will track and report training/validation loss and accuracy, and finally report the test accuracy on the clean CIFAR-10 test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "806cb5c5"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to define the hyperparameters for the training process and then instantiate the optimizer, loss function, and learning rate scheduler as per instructions 1-4 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcf6708c",
        "outputId": "12f9f263-697c-47d6-ce59-9a9f140f30b5"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# 1. Define hyperparameters for the training process\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-4\n",
        "STEP_SIZE = 15 # Number of epochs after which the learning rate is reduced\n",
        "GAMMA = 0.1 # Factor by which the learning rate is reduced\n",
        "\n",
        "print(f\"Hyperparameters defined: EPOCHS={EPOCHS}, LR={LEARNING_RATE}, Momentum={MOMENTUM}, Weight Decay={WEIGHT_DECAY}, Step Size={STEP_SIZE}, Gamma={GAMMA}\")\n",
        "\n",
        "# 2. Instantiate the SGD optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "print(\"SGD Optimizer instantiated.\")\n",
        "\n",
        "# 3. Instantiate the CrossEntropyLoss as the criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "print(\"CrossEntropyLoss criterion instantiated.\")\n",
        "\n",
        "# 4. Instantiate the StepLR learning rate scheduler\n",
        "scheduler = StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "print(\"StepLR scheduler instantiated.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters defined: EPOCHS=30, LR=0.01, Momentum=0.9, Weight Decay=0.0005, Step Size=15, Gamma=0.1\n",
            "SGD Optimizer instantiated.\n",
            "CrossEntropyLoss criterion instantiated.\n",
            "StepLR scheduler instantiated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8b07dc5"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the hyperparameters, optimizer, loss function, and scheduler are instantiated, I will implement the main training loop, which includes iterating through epochs, training, validating, stepping the scheduler, printing epoch-wise metrics, and finally evaluating on the clean test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2a924e9",
        "outputId": "c0a907ed-f21e-4412-81b4-b263b594f257"
      },
      "source": [
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# 5. Implement the main training loop\n",
        "print(\"Starting training loop...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    # a. Train for one epoch\n",
        "    train_loss, train_accuracy = train_one_epoch(model, train_loader_noisy, optimizer, criterion, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # c. Evaluate on the validation set\n",
        "    val_loss, val_accuracy = evaluate(model, val_loader_noisy, criterion, device)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # d. Step the learning rate scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # e. Print epoch results\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}: \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "print(\"Training loop finished.\")\n",
        "\n",
        "# 6. Evaluate the model on the clean test set\n",
        "test_loss, test_accuracy = evaluate(model, test_loader_clean, criterion, device)\n",
        "\n",
        "# 7. Print the final test loss and test accuracy\n",
        "print(f\"\\nFinal Test Loss on clean CIFAR-10: {test_loss:.4f}\")\n",
        "print(f\"Final Test Accuracy on clean CIFAR-10: {test_accuracy:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training loop...\n",
            "Epoch 1/30: Train Loss: 1.7105, Train Acc: 43.42% | Val Loss: 1.5844, Val Acc: 50.23%\n",
            "Epoch 2/30: Train Loss: 1.4872, Train Acc: 53.10% | Val Loss: 1.5640, Val Acc: 51.13%\n",
            "Epoch 3/30: Train Loss: 1.3800, Train Acc: 56.97% | Val Loss: 1.5932, Val Acc: 50.70%\n",
            "Epoch 4/30: Train Loss: 1.2862, Train Acc: 59.60% | Val Loss: 1.6332, Val Acc: 49.91%\n",
            "Epoch 5/30: Train Loss: 1.1897, Train Acc: 63.01% | Val Loss: 1.7250, Val Acc: 47.88%\n",
            "Epoch 6/30: Train Loss: 1.5902, Train Acc: 46.42% | Val Loss: 1.7567, Val Acc: 40.54%\n",
            "Epoch 7/30: Train Loss: 1.6297, Train Acc: 46.07% | Val Loss: 1.7049, Val Acc: 45.35%\n"
          ]
        }
      ]
    }
  ]
}